{{- define "kargo.krawler-cronjob" -}}
apiVersion: {{ .Capabilities.APIVersions.Has "batch/v1" | ternary "batch/v1" "batch/v1beta1" }}
kind: CronJob
metadata:
  name: {{ include "kargo.names.fullname" . }}
  namespace: {{ .Release.Namespace | quote }}
  labels: {{- include "kargo.labels.standard" . | nindent 4 }}
    {{- if .Values.forceRestart }}
    helm.sh/restartedAt: {{ now | unixEpoch |quote }}
    {{- end }}
    {{- if .Values.commonLabels }}
    {{- include "kargo.tplvalues.render" ( dict "value" .Values.commonLabels "context" $ ) | nindent 4 }}
    {{- end }}
  {{- if .Values.commonAnnotations }}
  annotations: {{- include "kargo.tplvalues.render" ( dict "value" .Values.commonAnnotations "context" $ ) | nindent 4 }}
  {{- end }}
spec:
  schedule: "{{ .Values.cron }}"
  suspend: {{ hasKey .Values "suspend" | ternary .Values.suspend false }}
  # It will run 1 at a time, but not more
  concurrencyPolicy: {{ .Values.concurrency | default "Forbid" }}
  successfulJobsHistoryLimit: {{ .Values.successfulJobsHistoryLimit | default 1 }} # Only keep one successful job
  failedJobsHistoryLimit: {{ .Values.failedJobsHistoryLimit | default 1 }} # Only keep one failed job
  jobTemplate:
    spec:
      # Number of times it will retry before it is considered failed
      backoffLimit: 0
      # Autoremove the job after that many seconds (failed or succeed), default 2days
      ttlSecondsAfterFinished: {{ .Values.autoRemoveJobAfter | default 172800 }}
      template:
        spec:
          securityContext:
            {{- if .Values.securityContext }} # Override allowed
            {{- toYaml .Values.securityContext | nindent 12 }}
            {{- else }}
            runAsNonRoot: true
            runAsUser: 1000 # That's 'node' user in base krawler docker image
            {{- end }}
          # Add a deadline so that if the job never finishes it will be stopped anyway
          # Could also be done using concurrencyPolicy = Replace but a deadline seems to be more flexible,
          # allowing a job to be temporarily slower than usual and to at least finish
          activeDeadlineSeconds: {{ .Values.deadline | default 3600 }}
          {{- with .Values.imagePullSecrets }}
          imagePullSecrets:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          containers:
            - name: {{ include "kargo.names.name" . }}
              image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
              imagePullPolicy: {{ ternary "Always" .Values.image.pullPolicy .Values.forceRestart }}
              env:          
                - name: SUBDOMAIN
                  value: {{ .Values.global.subdomain }}
                - name: CRON
                  value: ""
                - name: MESSAGE_TEMPLATE
                  value: "*<%= SUBDOMAIN %>*\nKrawler job *<%= jobId %>*: <%= error.message %>"
              {{- include "kargo.environment.render" (dict "env" .Values.env "rawEnv" .Values.rawEnv "context" $) | indent 16 }}
              {{- if .Values.dataVolume }}
              volumeMounts:
                - mountPath: {{ .Values.dataMountPath | default "/data" }}
                  name: job-data
                  {{- if .Values.dataSubPath }}
                  subPath: {{ include "kargo.tplvalues.render" ( dict "value" .Values.dataSubPath "context" $) }}
                  {{- end }}
              {{- end }}
              resources:
                {{- toYaml .Values.resources | nindent 16 }}
          # It won't restart on failure, next chance will be on next CRON
          restartPolicy: Never
          {{- if .Values.dataVolume }}
          volumes:
            - name: job-data
              {{- include "kargo.tplvalues.render" ( dict "value" .Values.dataVolume "context" $ ) | nindent 14 }}
          {{- end }}
{{- end -}}
